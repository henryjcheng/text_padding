{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Pipeline Testing\n",
    "Code to put embedded data through pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(list_to_pad, max_length, pad_dimension):\n",
    "    \"\"\"\n",
    "    This function takes a list and add list of zeros until max_length is reached.\n",
    "    The number of zeroes in added list is determined by pad_dimension, which is the \n",
    "    same as the dimension of the word2vec model.\n",
    "    This function is intended to handle one list only so it can be passed \n",
    "    into a dataframe as a lambda function.\n",
    "    \"\"\"\n",
    "    # find number of padding vector needed\n",
    "    num_pad = max_length - len(list_to_pad)\n",
    "\n",
    "    # vector_pad = np.zeros(pad_dimension)\n",
    "    vector_pad = np.asarray([0] * pad_dimension, dtype=np.float32)\n",
    "    vector_pad = [vector_pad]    # convert to list of np.ndarray so we can append together \n",
    "\n",
    "    iteration = 0\n",
    "    while iteration < num_pad:\n",
    "        list_to_pad = np.append(list_to_pad, vector_pad, axis=0)\n",
    "        iteration += 1\n",
    "    \n",
    "    return list_to_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/henry/data/School/master_thesis/env/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After embedding type: <class 'numpy.ndarray'>\n",
      "max length: 245\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 1. load dataset\n",
    "df = pd.read_csv('../data/ag_news/train.csv')\n",
    "\n",
    "## 2. apply tokenization and embedding\n",
    "df['text_token'] = df['Description'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "w2v = Word2Vec.load('../model/w2v/ag_news.model')\n",
    "df['embedding'] = df['text_token'].apply(lambda x: w2v[x])\n",
    "temp = df['embedding'][0]\n",
    "print(f'After embedding type: {type(temp)}')\n",
    "\n",
    "## 3. zero pad to max length\n",
    "df['text_length'] = df['text_token'].apply(lambda x: len(x))\n",
    "max_length = max(df['text_length'])\n",
    "\n",
    "print(f'max length: {max_length}')\n",
    "\n",
    "emb_dim = 50\n",
    "df['embedding'] = df['embedding'].apply(lambda x: zero_padding(x, max_length, emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "0    [[1.7402203, -0.44056985, 4.514008, 13.054911,...\n",
      "1    [[1.7402203, -0.44056985, 4.514008, 13.054911,...\n",
      "Name: embedding, dtype: object\n",
      "245\n",
      "tensor([[ 1.7402, -0.4406,  4.5140,  ..., -6.5562,  1.4705,  0.0400],\n",
      "        [ 1.4584,  1.9664,  0.2544,  ..., -6.6432, -1.0077, -3.2450],\n",
      "        [-0.0336,  0.1047, -0.0686,  ...,  0.0194, -0.0861,  0.0348],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "df['embedding'] = df['embedding'].apply(lambda x: zero_padding(x, max_length, emb_dim))\n",
    "\n",
    "test = df['embedding']\n",
    "\n",
    "print(type(test))\n",
    "print(test[:2])\n",
    "print(len(test[0]))\n",
    "print(torch.tensor(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_append = []\n",
    "\n",
    "for array in df['embedding']:\n",
    "    list_to_append.append(torch.tensor(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7402, -0.4406,  4.5140,  ..., -6.5562,  1.4705,  0.0400],\n",
      "        [ 1.4584,  1.9664,  0.2544,  ..., -6.6432, -1.0077, -3.2450],\n",
      "        [-0.0336,  0.1047, -0.0686,  ...,  0.0194, -0.0861,  0.0348],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 1.7402, -0.4406,  4.5140,  ..., -6.5562,  1.4705,  0.0400],\n",
      "        [ 1.4584,  1.9664,  0.2544,  ..., -6.6432, -1.0077, -3.2450],\n",
      "        [-0.2932,  0.5858,  0.4607,  ..., -0.2427, -0.5295,  0.5203],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 1.7402, -0.4406,  4.5140,  ..., -6.5562,  1.4705,  0.0400],\n",
      "        [ 1.4584,  1.9664,  0.2544,  ..., -6.6432, -1.0077, -3.2450],\n",
      "        [-0.1485,  0.4084, -0.2931,  ..., -0.1424,  0.1545, -0.1775],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 1.7402, -0.4406,  4.5140,  ..., -6.5562,  1.4705,  0.0400],\n",
      "        [ 1.4584,  1.9664,  0.2544,  ..., -6.6432, -1.0077, -3.2450],\n",
      "        [-0.4066,  0.2150,  0.7648,  ..., -0.4581, -0.1472, -0.1918],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 2.4550, -1.1441,  0.9696,  ..., -4.2619,  6.2025,  1.6525],\n",
      "        [ 1.4584,  1.9664,  0.2544,  ..., -6.6432, -1.0077, -3.2450],\n",
      "        [-0.0399,  0.0345, -0.0462,  ..., -0.0877, -0.0819,  0.0162],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for tensor in list_to_append[:5]:\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Dataloader\n",
    "Need to convert list of tensor to dataloader to feed through pytorch NN  \n",
    "Reference: https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    30000\n",
       "3    30000\n",
       "2    30000\n",
       "1    30000\n",
       "Name: Class Index, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class Index'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df['embedding'].tolist()[:1000]\n",
    "tensor_x = torch.tensor(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = df['Class Index'].tolist()[:1000]\n",
    "tensor_y = torch.tensor(train_y, dtype=torch.long)\n",
    "set(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = TensorDataset(tensor_x, tensor_y) # create your datset\n",
    "my_dataloader = DataLoader(my_dataset, batch_size=32) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Through NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=12250, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(245 * 50 * 1, 120) # 120 chosen randomly (< 245*50*1)\n",
    "        self.fc2 = nn.Linear(120, 50)           # 50 chosen randomly (< 50)\n",
    "        self.fc3 = nn.Linear(50, 4)             # 4 = number of classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 245 * 50 * 1)   # token length, w2v embedding dimension, channel\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "    \n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# train on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\ndevice: {device}')\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1\n",
      "\tbatch 1 loss:    0.06903758645057678\n",
      "\tbatch 2 loss:    0.12783314287662506\n",
      "\tbatch 3 loss:    0.06776856631040573\n",
      "\tbatch 4 loss:    0.017403386533260345\n",
      "\tbatch 5 loss:    -0.036544062197208405\n",
      "\tbatch 6 loss:    -0.014093950390815735\n",
      "\tbatch 7 loss:    0.03697685897350311\n",
      "\tbatch 8 loss:    0.01483616977930069\n",
      "\tbatch 9 loss:    0.017612140625715256\n",
      "\tbatch 10 loss:    0.029870377853512764\n",
      "\tbatch 11 loss:    0.005718499422073364\n",
      "\tbatch 12 loss:    0.02241034060716629\n",
      "\tbatch 13 loss:    -0.005740150809288025\n",
      "\tbatch 14 loss:    -0.03078792244195938\n",
      "\tbatch 15 loss:    0.03170553594827652\n",
      "\tbatch 16 loss:    0.07191524654626846\n",
      "\tbatch 17 loss:    0.03496114909648895\n",
      "\tbatch 18 loss:    0.06774330139160156\n",
      "\tbatch 19 loss:    0.0821763277053833\n",
      "\tbatch 20 loss:    0.1176464706659317\n",
      "\tbatch 21 loss:    0.06503792852163315\n",
      "\tbatch 22 loss:    0.13735079765319824\n",
      "\tbatch 23 loss:    0.13366611301898956\n",
      "\tbatch 24 loss:    0.12285207957029343\n",
      "\tbatch 25 loss:    0.4025611877441406\n",
      "\tbatch 26 loss:    0.19145898520946503\n",
      "\tbatch 27 loss:    0.15114594995975494\n",
      "\tbatch 28 loss:    0.2539651393890381\n",
      "\tbatch 29 loss:    0.22533366084098816\n",
      "\tbatch 30 loss:    0.26360708475112915\n",
      "\tbatch 31 loss:    0.18321175873279572\n",
      "\tbatch 32 loss:    -0.5023083090782166\n",
      "\n",
      "epoch 2\n",
      "\tbatch 1 loss:    0.06011577695608139\n",
      "\tbatch 2 loss:    0.11100319027900696\n",
      "\tbatch 3 loss:    0.003837764263153076\n",
      "\tbatch 4 loss:    0.013022774830460548\n",
      "\tbatch 5 loss:    0.011229950934648514\n",
      "\tbatch 6 loss:    -0.041404128074645996\n",
      "\tbatch 7 loss:    0.02223064750432968\n",
      "\tbatch 8 loss:    0.018978364765644073\n",
      "\tbatch 9 loss:    -0.04192422330379486\n",
      "\tbatch 10 loss:    0.022851355373859406\n",
      "\tbatch 11 loss:    0.04502812400460243\n",
      "\tbatch 12 loss:    -0.012446708977222443\n",
      "\tbatch 13 loss:    -0.01064891368150711\n",
      "\tbatch 14 loss:    -0.012906007468700409\n",
      "\tbatch 15 loss:    0.030896857380867004\n",
      "\tbatch 16 loss:    0.06526587158441544\n",
      "\tbatch 17 loss:    0.029241621494293213\n",
      "\tbatch 18 loss:    0.0665246918797493\n",
      "\tbatch 19 loss:    0.07657445967197418\n",
      "\tbatch 20 loss:    0.1125616729259491\n",
      "\tbatch 21 loss:    0.05294867604970932\n",
      "\tbatch 22 loss:    0.13134439289569855\n",
      "\tbatch 23 loss:    0.12469558417797089\n",
      "\tbatch 24 loss:    0.11547856032848358\n",
      "\tbatch 25 loss:    0.3884577453136444\n",
      "\tbatch 26 loss:    0.17903657257556915\n",
      "\tbatch 27 loss:    0.09785974770784378\n",
      "\tbatch 28 loss:    0.255577027797699\n",
      "\tbatch 29 loss:    0.20510922372341156\n",
      "\tbatch 30 loss:    0.2553410828113556\n",
      "\tbatch 31 loss:    0.1902422457933426\n",
      "\tbatch 32 loss:    -0.34571412205696106\n",
      "\n",
      "epoch 3\n",
      "\tbatch 1 loss:    0.04718909412622452\n",
      "\tbatch 2 loss:    0.08879049122333527\n",
      "\tbatch 3 loss:    0.016479671001434326\n",
      "\tbatch 4 loss:    -0.04297536611557007\n",
      "\tbatch 5 loss:    0.010944699868559837\n",
      "\tbatch 6 loss:    0.009886142797768116\n",
      "\tbatch 7 loss:    -0.007046036422252655\n",
      "\tbatch 8 loss:    0.009740181267261505\n",
      "\tbatch 9 loss:    -0.02162284404039383\n",
      "\tbatch 10 loss:    -0.03854062408208847\n",
      "\tbatch 11 loss:    0.03639177978038788\n",
      "\tbatch 12 loss:    0.03256197273731232\n",
      "\tbatch 13 loss:    -0.038066454231739044\n",
      "\tbatch 14 loss:    -0.01809099316596985\n",
      "\tbatch 15 loss:    0.025377877056598663\n",
      "\tbatch 16 loss:    0.054690927267074585\n",
      "\tbatch 17 loss:    0.025733664631843567\n",
      "\tbatch 18 loss:    0.06305944174528122\n",
      "\tbatch 19 loss:    0.07168063521385193\n",
      "\tbatch 20 loss:    0.10920040309429169\n",
      "\tbatch 21 loss:    0.04587806016206741\n",
      "\tbatch 22 loss:    0.13183194398880005\n",
      "\tbatch 23 loss:    0.1198052167892456\n",
      "\tbatch 24 loss:    0.11274027824401855\n",
      "\tbatch 25 loss:    0.37722674012184143\n",
      "\tbatch 26 loss:    0.19119800627231598\n",
      "\tbatch 27 loss:    0.1206844300031662\n",
      "\tbatch 28 loss:    0.24100081622600555\n",
      "\tbatch 29 loss:    0.19121426343917847\n",
      "\tbatch 30 loss:    0.2524894177913666\n",
      "\tbatch 31 loss:    0.1835961639881134\n",
      "\tbatch 32 loss:    0.1801193505525589\n",
      "\n",
      "epoch 4\n",
      "\tbatch 1 loss:    0.03402702510356903\n",
      "\tbatch 2 loss:    0.06667840480804443\n",
      "\tbatch 3 loss:    0.04022228717803955\n",
      "\tbatch 4 loss:    -0.019876793026924133\n",
      "\tbatch 5 loss:    -0.04517696797847748\n",
      "\tbatch 6 loss:    0.011510796844959259\n",
      "\tbatch 7 loss:    0.047194913029670715\n",
      "\tbatch 8 loss:    -0.016869939863681793\n",
      "\tbatch 9 loss:    -0.021768048405647278\n",
      "\tbatch 10 loss:    -0.015524975955486298\n",
      "\tbatch 11 loss:    -0.02142147719860077\n",
      "\tbatch 12 loss:    0.03223453462123871\n",
      "\tbatch 13 loss:    0.014239083975553513\n",
      "\tbatch 14 loss:    -0.04134912043809891\n",
      "\tbatch 15 loss:    0.02049534022808075\n",
      "\tbatch 16 loss:    0.04869624972343445\n",
      "\tbatch 17 loss:    0.023043915629386902\n",
      "\tbatch 18 loss:    0.05907231569290161\n",
      "\tbatch 19 loss:    0.06890557706356049\n",
      "\tbatch 20 loss:    0.10724644362926483\n",
      "\tbatch 21 loss:    0.03855500370264053\n",
      "\tbatch 22 loss:    0.1148129403591156\n",
      "\tbatch 23 loss:    0.11713510751724243\n",
      "\tbatch 24 loss:    0.10846984386444092\n",
      "\tbatch 25 loss:    0.3893906772136688\n",
      "\tbatch 26 loss:    0.16528001427650452\n",
      "\tbatch 27 loss:    0.12316474318504333\n",
      "\tbatch 28 loss:    0.2720385789871216\n",
      "\tbatch 29 loss:    0.18539272248744965\n",
      "\tbatch 30 loss:    0.2572271227836609\n",
      "\tbatch 31 loss:    0.25208574533462524\n",
      "\tbatch 32 loss:    -0.22142037749290466\n",
      "\n",
      "epoch 5\n",
      "\tbatch 1 loss:    0.03707127273082733\n",
      "\tbatch 2 loss:    0.06817727535963058\n",
      "\tbatch 3 loss:    -0.013356931507587433\n",
      "\tbatch 4 loss:    0.01418379694223404\n",
      "\tbatch 5 loss:    0.01158598717302084\n",
      "\tbatch 6 loss:    -0.045851245522499084\n",
      "\tbatch 7 loss:    0.015009038150310516\n",
      "\tbatch 8 loss:    0.007393136620521545\n",
      "\tbatch 9 loss:    -0.045426949858665466\n",
      "\tbatch 10 loss:    0.015085812658071518\n",
      "\tbatch 11 loss:    0.029727447777986526\n",
      "\tbatch 12 loss:    -0.02442401647567749\n",
      "\tbatch 13 loss:    -0.017498254776000977\n",
      "\tbatch 14 loss:    -0.018313363194465637\n",
      "\tbatch 15 loss:    0.02219168096780777\n",
      "\tbatch 16 loss:    0.0383283868432045\n",
      "\tbatch 17 loss:    0.015291959047317505\n",
      "\tbatch 18 loss:    0.060631006956100464\n",
      "\tbatch 19 loss:    0.06357589364051819\n",
      "\tbatch 20 loss:    0.12223932892084122\n",
      "\tbatch 21 loss:    0.03218687325716019\n",
      "\tbatch 22 loss:    0.12903718650341034\n",
      "\tbatch 23 loss:    0.10955873876810074\n",
      "\tbatch 24 loss:    0.11282563209533691\n",
      "\tbatch 25 loss:    0.38358259201049805\n",
      "\tbatch 26 loss:    0.193111389875412\n",
      "\tbatch 27 loss:    0.12213956564664841\n",
      "\tbatch 28 loss:    0.2442110776901245\n",
      "\tbatch 29 loss:    0.17685435712337494\n",
      "\tbatch 30 loss:    0.2526896893978119\n",
      "\tbatch 31 loss:    0.1913832724094391\n",
      "\tbatch 32 loss:    0.1811162233352661\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    print(f'\\nepoch {epoch + 1}')\n",
    "    for i, data in enumerate(my_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        print(f'\\tbatch {i+1} loss:    {loss.item()}')\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
